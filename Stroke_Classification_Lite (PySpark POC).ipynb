{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTop Row of DataFrame\u001b[0m\n",
      "+--------------+----+-----------------+----+------------+------+-------------+------------+----+---------------+------+---------+\n",
      "|Residence_type| age|avg_glucose_level| bmi|ever_married|gender|heart_disease|hypertension|  id| smoking_status|stroke|work_type|\n",
      "+--------------+----+-----------------+----+------------+------+-------------+------------+----+---------------+------+---------+\n",
      "|         Urban|67.0|           228.69|36.6|         Yes|  Male|            1|           0|9046|formerly smoked|     1|  Private|\n",
      "+--------------+----+-----------------+----+------------+------+-------------+------------+----+---------------+------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "\u001b[1mRecord Count Pre-drop: \u001b[0m 5110\n",
      "\u001b[1mRecord Count Post-drop: \u001b[0m 4909\n",
      "\u001b[1m\n",
      "Transforming Categorical Variables/\n",
      "Testing Original Column Against New Columns to Validate Consistency:\u001b[0m\n",
      "+-------------+-------+-------------+--------+--------+------------+\n",
      "|    work_type|Private|Self-employed|Govt_job|children|Never_worked|\n",
      "+-------------+-------+-------------+--------+--------+------------+\n",
      "|      Private|      1|            0|       0|       0|           0|\n",
      "|      Private|      1|            0|       0|       0|           0|\n",
      "|      Private|      1|            0|       0|       0|           0|\n",
      "|Self-employed|      0|            1|       0|       0|           0|\n",
      "|      Private|      1|            0|       0|       0|           0|\n",
      "+-------------+-------+-------------+--------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+------+-------+------------+---------------+\n",
      "| smoking_status|smokes|Unknown|never smoked|formerly smoked|\n",
      "+---------------+------+-------+------------+---------------+\n",
      "|formerly smoked|     0|      0|           0|              1|\n",
      "|   never smoked|     0|      0|           1|              0|\n",
      "|         smokes|     1|      0|           0|              0|\n",
      "|   never smoked|     0|      0|           1|              0|\n",
      "|formerly smoked|     0|      0|           0|              1|\n",
      "+---------------+------+-------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+--------------+----------------+------+--------------+------------+\n",
      "|Urban_Else_Rural|Residence_type|Male_Else_Female|gender|Ever_Married_2|ever_married|\n",
      "+----------------+--------------+----------------+------+--------------+------------+\n",
      "|               1|         Urban|               1|  Male|             1|         Yes|\n",
      "|               0|         Rural|               1|  Male|             1|         Yes|\n",
      "|               1|         Urban|               0|Female|             1|         Yes|\n",
      "|               0|         Rural|               0|Female|             1|         Yes|\n",
      "|               1|         Urban|               1|  Male|             1|         Yes|\n",
      "+----------------+--------------+----------------+------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, types\n",
    "class font:\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "df_json = spark.read.json('/Users/willdowdy/Desktop/Data_Options/stroke_data.json')\n",
    "\n",
    "print(font.BOLD + 'Top Row of DataFrame' + font.END)\n",
    "df_json.show(1)\n",
    "print(font.BOLD + 'Record Count Pre-drop: ' + font.END, df_json.count())\n",
    "df_json = df_json.na.drop()\n",
    "print(font.BOLD + 'Record Count Post-drop: ' + font.END, df_json.count())\n",
    "\n",
    "import pyspark.sql.functions as F \n",
    "categ = df_json.select('work_type').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "exprs = [F.when(F.col('work_type') == cat,1).otherwise(0)\\\n",
    "            .alias(str(cat)) for cat in categ]\n",
    "df_json = df_json.select(exprs+df_json.columns)\n",
    "\n",
    "print(font.BOLD + '\\nTransforming Categorical Variables/\\nTesting Original Column Against New Columns to Validate Consistency:' + font.END)\n",
    "df_json.select(['work_type','Private','Self-employed','Govt_job','children','Never_worked']).show(5)\n",
    "\n",
    "categ = df_json.select('smoking_status').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "exprs = [F.when(F.col('smoking_status') == cat,1).otherwise(0)\\\n",
    "            .alias(str(cat)) for cat in categ]\n",
    "df_json = df_json.select(exprs+df_json.columns)\n",
    "\n",
    "df_json.select(['smoking_status','smokes','Unknown','never smoked','formerly smoked']).show(5)\n",
    "\n",
    "df_json = df_json.withColumn('Urban_Else_Rural', F.when(F.col(\"Residence_type\") == 'Rural', 0).otherwise(1))\n",
    "df_json = df_json.withColumn('Male_Else_Female', F.when(F.col(\"gender\") == 'Female', 0).otherwise(1))\n",
    "df_json = df_json.withColumn('Ever_Married_2', F.when(F.col(\"ever_married\") == 'No', 0).otherwise(1))\n",
    "\n",
    "df_json.select('Urban_Else_Rural', 'Residence_type','Male_Else_Female','gender','Ever_Married_2','ever_married').show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNormalizing Features of Varied Scales/Magnitudes - Setting Scale Between 0 and 1\u001b[0m\n",
      "+----+----------+----+----------+-----------------+------------------------+\n",
      "| age|age_Scaled| bmi|bmi_Scaled|avg_glucose_level|avg_glucose_level_Scaled|\n",
      "+----+----------+----+----------+-----------------+------------------------+\n",
      "|67.0|     0.817|36.6|     0.301|           228.69|                   0.801|\n",
      "|80.0|     0.976|32.5|     0.254|           105.92|                   0.235|\n",
      "|49.0|     0.597|34.4|     0.276|           171.23|                   0.536|\n",
      "|79.0|     0.963|24.0|     0.157|           174.12|                   0.549|\n",
      "|81.0|     0.988|29.0|     0.214|           186.21|                   0.605|\n",
      "+----+----------+----+----------+-----------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json = df_json.drop('work_type','smoking_status','id','Residence_type','gender','ever_married')\n",
    "df_json = df_json.select([F.col(c).cast(\"double\") for c in df_json.columns])\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "scale_df = df_json.select('age','avg_glucose_level','bmi')\n",
    "\n",
    "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "\n",
    "for i in df_json.columns:\n",
    "    if i in ['age','bmi','avg_glucose_level']:\n",
    "        assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "        scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "        pipeline = Pipeline(stages=[assembler, scaler])\n",
    "        df_json = pipeline.fit(df_json).transform(df_json).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "        \n",
    "print(font.BOLD + \"Normalizing Features of Varied Scales/Magnitudes - Setting Scale Between 0 and 1\" + font.END) \n",
    "df_json.select('age','age_Scaled','bmi','bmi_Scaled','avg_glucose_level','avg_glucose_level_Scaled').show(5)\n",
    "\n",
    "df_json = df_json.drop('age','avg_glucose_level','bmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRunning Cross-Validated Random Forest Model\u001b[0m\n",
      "AUC Score: 0.7835349092908191\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "test_df = df_json.select('formerly smoked','age_Scaled','Urban_Else_Rural','Male_Else_Female','stroke')\n",
    "assemblerInputs = ['formerly smoked','age_Scaled','Urban_Else_Rural','Male_Else_Female']\n",
    "vector_assembler = VectorAssembler(inputCols = assemblerInputs, outputCol = 'features')\n",
    "assembler_temp = vector_assembler.transform(test_df)\n",
    "\n",
    "assembler = assembler_temp.drop('formerly smoked','age_Scaled','Urban_Else_Rural','Male_Else_Female')\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "label_indexer = StringIndexer(inputCol = \"stroke\", outputCol = \"label\")\n",
    "ml_dataset = label_indexer.fit(assembler).transform(assembler)\n",
    "temp = ml_dataset.select('stroke','label','features')\n",
    "\n",
    "zeros = temp.filter(\"label==0.0\")\n",
    "ones = temp.filter(\"label==1.0\")\n",
    "from pyspark.sql.functions import rand\n",
    "zeros = zeros.orderBy(rand()).limit(487)\n",
    "unionDF = zeros.union(ones).orderBy(rand())\n",
    "\n",
    "training, test = unionDF.randomSplit([0.8,0.2])\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "rf = RandomForestClassifier(labelCol = \"label\", featuresCol= \"features\")\n",
    "bc_evaluator = BinaryClassificationEvaluator(labelCol = \"label\", rawPredictionCol = \"prediction\")\n",
    "rfparamGrid = (ParamGridBuilder().addGrid(rf.maxDepth, [2, 5, 10]).addGrid(rf.maxBins, [5, 10, 20]).addGrid(rf.numTrees, [5, 20, 50]).build())\n",
    "\n",
    "rfcv = CrossValidator(estimator = rf, estimatorParamMaps = rfparamGrid, evaluator = bc_evaluator, numFolds = 10)\n",
    "\n",
    "rfcvModel = rfcv.fit(training)\n",
    "rfpredictions = rfcvModel.transform(test)\n",
    "print(font.BOLD + \"Running Cross-Validated Random Forest Model\" + font.END)\n",
    "print('AUC Score:', bc_evaluator.evaluate(rfpredictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
